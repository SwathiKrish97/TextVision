{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANOEjqnBOuQN",
        "outputId": "d38ad543-0c9d-4e57-9238-b5f01637c8ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ANIMOV-512x Fine-tuned Model**"
      ],
      "metadata": {
        "id": "0Ez6Gz7UVUrK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# Define function to combine frames in a single horizontal row\n",
        "def combine_frames_horizontally(frames):\n",
        "    return np.hstack(frames)\n",
        "\n",
        "# Specify the model folder, video path, and prompt\n",
        "model_folder = \"/content/drive/MyDrive/CLIPSIM/FinetunedModel/ANIMOV-512x\"  # Updated path for Fine-tuned Model\n",
        "video_path = os.path.join(model_folder, \"Data.mp4\")\n",
        "prompt = \"man doing pushups\"\n",
        "\n",
        "# Initialize frame capture\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "frames = []\n",
        "frame_count = 0\n",
        "\n",
        "# Check if the video was opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error opening video file\")\n",
        "\n",
        "# Read frames until video ends or reach desired count\n",
        "while cap.isOpened() and frame_count < 100:\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        resized_frame = cv2.resize(frame, (256, 256))  # Resize frames to keep them manageable\n",
        "        frames.append(resized_frame)\n",
        "        frame_count += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Combine frames into a single horizontal image and save it in the model's folder\n",
        "combined_image_path = os.path.join(model_folder, \"combined_frames_output.jpg\")\n",
        "if len(frames) > 0:\n",
        "    combined_image = combine_frames_horizontally(frames[:100])  # Use the first 100 frames\n",
        "    cv2.imwrite(combined_image_path, combined_image)\n",
        "    print(f'Combined image of multiple frames saved successfully in {combined_image_path}')\n",
        "else:\n",
        "    print('No frames to combine.')\n",
        "\n",
        "# CLIP model for CLIPSIM score calculation\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Check if the combined image was created before using it\n",
        "if os.path.exists(combined_image_path):\n",
        "    image = Image.open(combined_image_path)\n",
        "    inputs = processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Model inference to get embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        image_embeds = outputs.image_embeds\n",
        "        text_embeds = outputs.text_embeds\n",
        "\n",
        "    # Normalize embeddings and calculate cosine similarity\n",
        "    image_embeds = F.normalize(image_embeds, p=2, dim=-1)\n",
        "    text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
        "    clip_score = torch.matmul(image_embeds, text_embeds.T).item()\n",
        "\n",
        "    print(f\"CLIP Score for (ANIMOV-512x Fine-tuned Model): {clip_score}\")\n",
        "else:\n",
        "    print(\"Combined image was not created due to insufficient frames.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ11ZKjaRl4m",
        "outputId": "7adf5c34-3525-49c9-d78e-86af23f2ed99"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined image of multiple frames saved successfully in /content/drive/MyDrive/CLIPSIM/FinetunedModel/ANIMOV-512x/combined_frames_output.jpg\n",
            "CLIP Score for (ANIMOV-512x Fine-tuned Model): 0.2550389766693115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **POTAT1 Fine-tuned Model**"
      ],
      "metadata": {
        "id": "VnQp8QUAWUNo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# Define function to combine frames in a single horizontal row\n",
        "def combine_frames_horizontally(frames):\n",
        "    return np.hstack(frames)\n",
        "\n",
        "# Specify the model folder, video path, and prompt for the Fine-tuned Model\n",
        "model_folder = \"/content/drive/MyDrive/CLIPSIM/FinetunedModel/POTAT1\"  # Fine-tuned model path\n",
        "video_path = os.path.join(model_folder, \"Data.mp4\")\n",
        "prompt = \"man doing pushups\"\n",
        "\n",
        "# Initialize frame capture\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "frames = []\n",
        "frame_count = 0\n",
        "\n",
        "# Check if the video was opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error opening video file\")\n",
        "\n",
        "# Read frames until video ends or reach desired count\n",
        "while cap.isOpened() and frame_count < 100:\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        resized_frame = cv2.resize(frame, (256, 256))  # Resize frames to keep them manageable\n",
        "        frames.append(resized_frame)\n",
        "        frame_count += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Combine frames into a single horizontal image and save it in the model's folder\n",
        "combined_image_path = os.path.join(model_folder, \"combined_frames_output.jpg\")\n",
        "if len(frames) > 0:\n",
        "    combined_image = combine_frames_horizontally(frames[:100])  # Use the first 100 frames\n",
        "    cv2.imwrite(combined_image_path, combined_image)\n",
        "    print(f'Combined image of multiple frames saved successfully in {combined_image_path}')\n",
        "else:\n",
        "    print('No frames to combine.')\n",
        "\n",
        "# CLIP model for CLIPSIM score calculation\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Check if the combined image was created before using it\n",
        "if os.path.exists(combined_image_path):\n",
        "    image = Image.open(combined_image_path)\n",
        "    inputs = processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Model inference to get embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        image_embeds = outputs.image_embeds\n",
        "        text_embeds = outputs.text_embeds\n",
        "\n",
        "    # Normalize embeddings and calculate cosine similarity\n",
        "    image_embeds = F.normalize(image_embeds, p=2, dim=-1)\n",
        "    text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
        "    clip_score = torch.matmul(image_embeds, text_embeds.T).item()\n",
        "\n",
        "    print(f\"CLIP Score for (POTAT1 Fine-tuned Model): {clip_score}\")\n",
        "else:\n",
        "    print(\"Combined image was not created due to insufficient frames.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8S7Z4cTShwo",
        "outputId": "4c104be9-a66e-4c10-8de7-358ab31bf736"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined image of multiple frames saved successfully in /content/drive/MyDrive/CLIPSIM/FinetunedModel/POTAT1/combined_frames_output.jpg\n",
            "CLIP Score for (POTAT1 Fine-tuned Model): 0.2913568615913391\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ZEROSCOPE Fine-tuned Model**"
      ],
      "metadata": {
        "id": "ooKsvaTqWZ9W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# Define function to combine frames in a single horizontal row\n",
        "def combine_frames_horizontally(frames):\n",
        "    return np.hstack(frames)\n",
        "\n",
        "# Specify the model folder, video path, and prompt for the Fine-tuned Model\n",
        "model_folder = \"/content/drive/MyDrive/CLIPSIM/FinetunedModel/ZEROSCOPE\"  # Fine-tuned model path\n",
        "video_path = os.path.join(model_folder, \"Data.mp4\")\n",
        "prompt = \"Man doing pushups\"\n",
        "\n",
        "# Initialize frame capture\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "frames = []\n",
        "frame_count = 0\n",
        "\n",
        "# Check if the video was opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error opening video file\")\n",
        "\n",
        "# Read frames until video ends or reach desired count\n",
        "while cap.isOpened() and frame_count < 100:\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        resized_frame = cv2.resize(frame, (256, 256))  # Resize frames to keep them manageable\n",
        "        frames.append(resized_frame)\n",
        "        frame_count += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Combine frames into a single horizontal image and save it in the model's folder\n",
        "combined_image_path = os.path.join(model_folder, \"combined_frames_output.jpg\")\n",
        "\n",
        "if len(frames) > 0:\n",
        "    combined_image = combine_frames_horizontally(frames[:100])  # Use the first 100 frames\n",
        "    cv2.imwrite(combined_image_path, combined_image)\n",
        "    print(f'Combined image of multiple frames saved successfully in {combined_image_path}')\n",
        "else:\n",
        "    print('No frames to combine.')\n",
        "\n",
        "# CLIP model for CLIPSIM score calculation\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Check if the combined image was created before using it\n",
        "if os.path.exists(combined_image_path):\n",
        "    image = Image.open(combined_image_path)\n",
        "    inputs = processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Model inference to get embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        image_embeds = outputs.image_embeds\n",
        "        text_embeds = outputs.text_embeds\n",
        "\n",
        "    # Normalize embeddings and calculate cosine similarity\n",
        "    image_embeds = F.normalize(image_embeds, p=2, dim=-1)\n",
        "    text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
        "    clip_score = torch.matmul(image_embeds, text_embeds.T).item()\n",
        "\n",
        "    print(f\"CLIP Score for (ZEROSCOPE Fine-tuned Model): {clip_score}\")\n",
        "else:\n",
        "    print(\"Combined image was not created due to insufficient frames.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-4VA2_vS51w",
        "outputId": "1dc47230-eb5a-4c48-8e59-7f4b60668d56"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined image of multiple frames saved successfully in /content/drive/MyDrive/CLIPSIM/FinetunedModel/ZEROSCOPE/combined_frames_output.jpg\n",
            "CLIP Score for (ZEROSCOPE Fine-tuned Model): 0.33747565746307373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DAMO VILAB Fine-tuned Model**"
      ],
      "metadata": {
        "id": "Px8gWXpcWll5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "# Define function to combine frames in a single horizontal row\n",
        "def combine_frames_horizontally(frames):\n",
        "    return np.hstack(frames)\n",
        "\n",
        "# Specify the model folder, video path, and prompt for the Fine-tuned Model\n",
        "model_folder = \"/content/drive/MyDrive/CLIPSIM/FinetunedModel/DAMO VILAB\"  # Fine-tuned model path\n",
        "video_path = os.path.join(model_folder, \"Data.mp4\")\n",
        "prompt = \"Man doing pushups\"\n",
        "\n",
        "# Initialize frame capture\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "frames = []\n",
        "frame_count = 0\n",
        "\n",
        "# Check if the video was opened successfully\n",
        "if not cap.isOpened():\n",
        "    print(\"Error opening video file\")\n",
        "\n",
        "# Read frames until video ends or reach desired count\n",
        "while cap.isOpened() and frame_count < 100:\n",
        "    ret, frame = cap.read()\n",
        "    if ret:\n",
        "        resized_frame = cv2.resize(frame, (256, 256))  # Resize frames to keep them manageable\n",
        "        frames.append(resized_frame)\n",
        "        frame_count += 1\n",
        "    else:\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "\n",
        "# Combine frames into a single horizontal image and save it in the model's folder\n",
        "combined_image_path = os.path.join(model_folder, \"combined_frames_output.jpg\")\n",
        "if len(frames) > 0:\n",
        "    combined_image = combine_frames_horizontally(frames[:100])  # Use the first 100 frames\n",
        "    cv2.imwrite(combined_image_path, combined_image)\n",
        "    print(f'Combined image of multiple frames saved successfully in {combined_image_path}')\n",
        "else:\n",
        "    print('No frames to combine.')\n",
        "\n",
        "# CLIP model for CLIPSIM score calculation\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Check if the combined image was created before using it\n",
        "if os.path.exists(combined_image_path):\n",
        "    image = Image.open(combined_image_path)\n",
        "    inputs = processor(text=[prompt], images=image, return_tensors=\"pt\", padding=True)\n",
        "\n",
        "    # Model inference to get embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        image_embeds = outputs.image_embeds\n",
        "        text_embeds = outputs.text_embeds\n",
        "\n",
        "    # Normalize embeddings and calculate cosine similarity\n",
        "    image_embeds = F.normalize(image_embeds, p=2, dim=-1)\n",
        "    text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
        "    clip_score = torch.matmul(image_embeds, text_embeds.T).item()\n",
        "\n",
        "    print(f\"CLIP Score for (DAMO VILAB Fine-tuned Model): {clip_score}\")\n",
        "else:\n",
        "    print(\"Combined image was not created due to insufficient frames.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfNSetANVkKE",
        "outputId": "0d7bba0d-10c4-4fa6-d60b-75cd1a0f0e19"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined image of multiple frames saved successfully in /content/drive/MyDrive/CLIPSIM/FinetunedModel/DAMO VILAB/combined_frames_output.jpg\n",
            "CLIP Score for (DAMO VILAB Fine-tuned Model): 0.304095059633255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CLIPSim Score Calculation for All Models **\n"
      ],
      "metadata": {
        "id": "taEPNn-E9uxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "import torch\n",
        "from PIL import Image\n",
        "import torch.nn.functional as F\n",
        "from pathlib import Path\n",
        "import os\n",
        "import csv\n",
        "\n",
        "# Load pre-trained CLIP model and processor\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Function to combine frames in a single horizontal row\n",
        "def combine_frames_horizontally(frames):\n",
        "    return np.hstack(frames)\n",
        "\n",
        "# Function to extract 100 frames and save combined frames image\n",
        "def extract_and_combine_frames(video_path, combined_image_path):\n",
        "    cap = cv2.VideoCapture(str(video_path))\n",
        "    frames = []\n",
        "    frame_count = 0\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(f\"Error opening video file {video_path}\")\n",
        "        return None\n",
        "\n",
        "    # Read frames until video ends or reach desired count\n",
        "    while cap.isOpened() and frame_count < 100:\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            resized_frame = cv2.resize(frame, (256, 256))  # Resize frames to keep them manageable\n",
        "            frames.append(resized_frame)\n",
        "            frame_count += 1\n",
        "        else:\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Combine frames into a single horizontal image and save\n",
        "    if len(frames) > 0:\n",
        "        combined_image = combine_frames_horizontally(frames[:100])  # Use the first 100 frames\n",
        "        cv2.imwrite(combined_image_path, combined_image)\n",
        "        print(f'Combined image of multiple frames saved successfully in {combined_image_path}')\n",
        "        return combined_image_path\n",
        "    else:\n",
        "        print(f'No frames to combine for {video_path}')\n",
        "        return None\n",
        "\n",
        "# Function to calculate CLIP similarity for combined image and text prompt\n",
        "def calculate_clipsim(combined_image_path, text_prompt):\n",
        "    if os.path.exists(combined_image_path):\n",
        "        image = Image.open(combined_image_path)\n",
        "        inputs = processor(text=[text_prompt], images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "        # Model inference to get embeddings\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            image_embeds = outputs.image_embeds\n",
        "            text_embeds = outputs.text_embeds\n",
        "\n",
        "        # Normalize embeddings and calculate cosine similarity\n",
        "        image_embeds = F.normalize(image_embeds, p=2, dim=-1)\n",
        "        text_embeds = F.normalize(text_embeds, p=2, dim=-1)\n",
        "        clip_score = torch.matmul(image_embeds, text_embeds.T).item()\n",
        "\n",
        "        return clip_score\n",
        "    else:\n",
        "        print(f\"Combined image not found at {combined_image_path}\")\n",
        "        return None\n",
        "\n",
        "# Model directories and prompts\n",
        "model_directories = {\n",
        "    \"ANIMOV-512x\": \"man doing pushups\",\n",
        "    \"POTAT1\": \"man doing pushups\",\n",
        "    \"ZEROSCOPE\": \"man doing pushups\",\n",
        "    \"DAMO VILAB\": \"man doing pushups\"\n",
        "}\n",
        "\n",
        "# Directory containing model folders\n",
        "base_directory = '/content/drive/MyDrive/CLIPSIM/FinetunedModel'\n",
        "results = []\n",
        "\n",
        "# Iterate over each model folder and calculate CLIPSim score\n",
        "for model_name, prompt in model_directories.items():\n",
        "    model_folder = os.path.join(base_directory, model_name)\n",
        "    video_path = os.path.join(model_folder, \"Data.mp4\")  # Ensure each model folder has \"Data.mp4\"\n",
        "    combined_image_path = os.path.join(model_folder, \"combined_frames_output.jpg\")\n",
        "\n",
        "    # Extract frames and save combined image\n",
        "    combined_image = extract_and_combine_frames(video_path, combined_image_path)\n",
        "\n",
        "    if combined_image:\n",
        "        # Calculate CLIPSim score\n",
        "        clipsim_score = calculate_clipsim(combined_image, prompt)\n",
        "        if clipsim_score is not None:\n",
        "            results.append({\"model\": model_name, \"CLIPSim_score\": clipsim_score})\n",
        "            print(f\"CLIPSim score for {model_name} (Fine-tuned Model): {clipsim_score:.4f}\")\n",
        "\n",
        "# Save results to a CSV file\n",
        "output_csv_path = '/content/drive/MyDrive/CLIPSim_scores_finetuned_models.csv'\n",
        "with open(output_csv_path, mode='w', newline='') as file:\n",
        "    writer = csv.DictWriter(file, fieldnames=[\"model\", \"CLIPSim_score\"])\n",
        "    writer.writeheader()\n",
        "    writer.writerows(results)\n",
        "\n",
        "print(f\"Results saved to {output_csv_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTnc0JKnne25",
        "outputId": "cb8f1886-f12e-497b-cdde-cd4ec1353ed4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined image of multiple frames saved successfully in /content/drive/MyDrive/CLIPSIM/FinetunedModel/ANIMOV-512x/combined_frames_output.jpg\n",
            "CLIPSim score for ANIMOV-512x (Fine-tuned Model): 0.2550\n",
            "Combined image of multiple frames saved successfully in /content/drive/MyDrive/CLIPSIM/FinetunedModel/POTAT1/combined_frames_output.jpg\n",
            "CLIPSim score for POTAT1 (Fine-tuned Model): 0.2914\n",
            "Combined image of multiple frames saved successfully in /content/drive/MyDrive/CLIPSIM/FinetunedModel/ZEROSCOPE/combined_frames_output.jpg\n",
            "CLIPSim score for ZEROSCOPE (Fine-tuned Model): 0.3375\n",
            "Combined image of multiple frames saved successfully in /content/drive/MyDrive/CLIPSIM/FinetunedModel/DAMO VILAB/combined_frames_output.jpg\n",
            "CLIPSim score for DAMO VILAB (Fine-tuned Model): 0.3041\n",
            "Results saved to /content/drive/MyDrive/CLIPSim_scores_finetuned_models.csv\n"
          ]
        }
      ]
    }
  ]
}